import logging
from pathlib import Path
import numpy as np
import torch
import json
import os
import time
import traceback
import cv2 as cv

# --- Dependency Imports ---
try:
    from src.data_io import ImageData
    from src.medsam_segmentation import MedSAMTool
except ImportError as import_err:
    print(f"FATAL ERROR: Failed to import required modules (ImageData, Tool): {import_err}")
    print(traceback.format_exc())
    exit(1)


# --- Configuration (Replaced by run_pipeline_prompt) ---
gpu_id = {gpu_id}
seed = {seed} # Seed is now an integer, no quotes needed
data_path = r"{dataset_path}"
function_bank_path = r"{function_bank_path}"
script_start_time = time.time()

# --- Saving Function Definition ---
def save_results_to_json(file_path, results_dict):
    '''Appends results dictionary to the JSON file specified.'''
    print(f"(Saver) Attempting to save results to: {file_path}")
    results_dict['timestamp'] = results_dict.get('timestamp', time.time())
    try:
        output_dir = os.path.dirname(file_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            print(f"(Saver) Created directory: {output_dir}")
        try:
            if not os.path.exists(file_path):
                 print(f"(Saver) Warning: JSON file '{file_path}' not found. Creating empty list.")
                 existing_data = []
                 with open(file_path, 'w') as f: json.dump(existing_data, f)
            with open(file_path, 'r') as f:
                existing_data = json.load(f)
            if not isinstance(existing_data, list):
                 print(f"(Saver) Warning: JSON file '{file_path}' did not contain a list. Re-initializing.")
                 existing_data = []
        except (FileNotFoundError, json.JSONDecodeError):
             print(f"(Saver) Warning: Could not read or decode existing JSON '{file_path}'. Starting new list.")
             existing_data = []
        except Exception as read_err:
             print(f"(Saver) Error reading existing file {file_path}: {read_err}. Starting new list.")
             existing_data = []
        existing_data.append(results_dict)
        with open(file_path, 'w') as f:
            json.dump(existing_data, f, indent=4)
        print(f"(Saver) Successfully appended results to {file_path}")
    except Exception as e:
        print(f"(Saver) !!! ERROR saving results to {file_path}: {e}\\n{traceback.format_exc()}")


# --- Setup Logging --- # Optional.  Can be removed
logger = None
# Use function_bank_path (already defined above) to derive log path
log_file_path = os.path.join(os.path.dirname(function_bank_path), "pipeline_run.log")
output_dir_for_log = os.path.dirname(log_file_path)
if output_dir_for_log and not os.path.exists(output_dir_for_log):
     try:
         os.makedirs(output_dir_for_log, exist_ok=True)
         print(f"Created log directory: {output_dir_for_log}")
     except OSError as e:
         print(f"Warning: Could not create log directory {output_dir_for_log}: {e}")
         log_file_path = "pipeline_run.log" # Fallback

print(f"Setting up logging to file: {log_file_path}")
try:
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file_path),
            logging.StreamHandler()
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    logger.info("Logging configured successfully.")
except Exception as log_e:
     print(f"Error configuring logging: {log_e}. Using basic print statements.")
     class PrintLogger:
         def info(self, msg, *args): print("INFO: " + (msg % args if args else msg))
         def warning(self, msg, *args): print("WARNING: " + (msg % args if args else msg))
         def error(self, msg, *args): print("ERROR: " + (msg % args if args else msg))
         def exception(self, msg, *args): print("EXCEPTION: " + (msg % args if args else msg) + f"\\n{traceback.format_exc()}")
     logger = PrintLogger()


# --- Initialize variables ---
overall_metrics = None
preprocessing_time = None
prediction_time = None
metrics_time = None
pipeline_error = None
pipeline_traceback = None
preprocess_images = None

# --- Generated Preprocessing Function Definition ---
# This placeholder will be replaced by the executor
{_PREPROCESSING_FUNCTION_PLACEHOLDER}
# --- End of Generated Code ---

try:
    # --- Check if function exists ---
    if 'preprocess_images' not in globals() or not callable(preprocess_images):
        err_msg = "FATAL: preprocess_images function not defined correctly by generated code."
        logger.error(err_msg)
        raise NameError(err_msg)
    else:
        logger.info("preprocess_images function appears to be defined.")

    # --- Set random seeds ---
    logger.info(f"Setting random seeds using integer seed: {seed}")
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)

    # --- Initialize segmenter tool ---
    logger.info("Initializing MedSAMTool...")
    segmenter = MedSAMTool(gpu_id=gpu_id, checkpoint_path="/workspace/data/medsam_vit_b.pth")
    logger.info(f"Loading data from {data_path}...")
    raw_images, boxes, masks = segmenter.loadData(data_path)
    logger.info(f"Data loaded: {len(raw_images)} images.")

    # --- Prepare ImageData ---
    batch_size = 16
    images = ImageData(raw=raw_images,
                batch_size=batch_size,
                image_ids=[i for i in range(len(raw_images))],
                masks=masks,
                predicted_masks=masks)

    # --- Apply Preprocessing ---
    logger.info("Applying preprocess_images function...")
    preprocessing_start_time = time.time()
    try:
        images = preprocess_images(images)
        preprocessing_time = time.time() - preprocessing_start_time
        logger.info(f"Preprocessing finished in {preprocessing_time:.2f} seconds.")
        if not isinstance(images, ImageData):
            raise TypeError(f"Preprocessing function returned type {type(images)}, expected ImageData.")
    except Exception as preprocess_error:
        logger.error(f"!!! ERROR occurred inside the dynamically generated preprocess_images function:")
        logger.exception(preprocess_error)
        raise preprocess_error

    # --- Run Segmenter ---
    logger.info("Running segmentation prediction...")
    prediction_start_time = time.time()
    pred_masks = segmenter.predict(images, boxes, used_for_baseline=False)
    prediction_time = time.time() - prediction_start_time
    logger.info(f"Segmentation prediction finished in {prediction_time:.2f} seconds.")

    # --- Calculate Metrics ---
    logger.info("Calculating metrics...")
    metrics_start_time = time.time()
    overall_metrics = segmenter.evaluate(pred_masks, images.masks)
    metrics_time = time.time() - metrics_start_time
    logger.info(f"Metrics calculation finished in {metrics_time:.2f} seconds.")

    # --- Log Metrics ---
    logger.info("Overall metrics: %s", json.dumps(overall_metrics if overall_metrics is not None else 'N/A', indent=2))

    # --- CHANGE: Save Results ONLY on SUCCESS (End of Try Block) ---
    # Check if metrics were actually calculated (not None)
    if overall_metrics is not None:
        logger.info("Saving successful results to function bank...")
        # Access the code string variable injected by the executor
        preprocessing_code_str = globals().get("_GENERATED_CODE_STRING", None)
        results_to_save = {
            'preprocessing_function': preprocessing_code_str,
            'overall_metrics': overall_metrics, # Known to be not None here
            'seed': seed,
            'timestamp': script_start_time,
            'preprocessing_time': preprocessing_time,
            'prediction_time': prediction_time,
            'metrics_time': metrics_time,
            'error': None, # Explicitly None on success
            'traceback': None # Explicitly None on success
        }
        # Call the save function defined earlier
        save_results_to_json(function_bank_path, results_to_save)
    else:
        # This case shouldn't normally be reached if evaluate() works,
        # but good to log if metrics end up None without an exception.
        logger.warning("Pipeline seemed to complete, but no metrics were generated. Not saving to bank.")

except Exception as e:
     # Errors caught here (from function check, pipeline steps, or re-raised preprocess error)
     pipeline_error = f"Error during pipeline execution: {str(e)}"
     pipeline_traceback = traceback.format_exc()
     logger.error(f"!!! ERROR during pipeline execution:")
     logger.exception(e)

finally:
    # --- CHANGE: Finally block now only handles logging end and exit code ---
    logger.info("--- Run Finished ---")
    # Check if an error was recorded in the except block
    if 'pipeline_error' in locals() and pipeline_error:
        logger.warning("Exiting with error code 1 due to pipeline failure.")
        exit(1)
    # Otherwise, exit normally (exit code 0)
