import logging
from pathlib import Path
import numpy as np
import torch
import json
import os
import time
import traceback
import re
import inspect

# --- Dependency Imports ---
try:
    from src.data_io import ImageData
    from src.cellpose_segmentation import CellposeTool
    # import cv2 as cv # Uncomment if needed
except ImportError as import_err:
    print(f"FATAL ERROR: Failed to import required modules (ImageData, CellposeTool): {import_err}")
    print(traceback.format_exc())
    exit(1)


# --- Configuration (Replaced by run_pipeline_prompt) ---
gpu_id = {gpu_id}
dataset_size = {dataset_size}
batch_size = {batch_size}
seed = {seed} # Seed is now an integer, no quotes needed
k = {sample_k}
group_baseline = {baseline_metric_value}
advantage_enabled = {advantage_enabled}
data_path = r"{dataset_path}"
function_bank_path = r"{function_bank_path}"
script_start_time = time.time()

# --- Saving Function Definition ---
def save_results_to_json(file_path, results_dict):
    '''Appends results dictionary to the JSON file specified.'''
    print(f"(Saver) Attempting to save results to: {file_path}")
    results_dict['timestamp'] = results_dict.get('timestamp', time.time())
    try:
        output_dir = os.path.dirname(file_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            print(f"(Saver) Created directory: {output_dir}")
        try:
            if not os.path.exists(file_path):
                 print(f"(Saver) Warning: JSON file '{file_path}' not found. Creating empty list.")
                 existing_data = []
                 with open(file_path, 'w') as f: json.dump(existing_data, f)
            with open(file_path, 'r') as f:
                existing_data = json.load(f)
            if not isinstance(existing_data, list):
                 print(f"(Saver) Warning: JSON file '{file_path}' did not contain a list. Re-initializing.")
                 existing_data = []
        except (FileNotFoundError, json.JSONDecodeError):
             print(f"(Saver) Warning: Could not read or decode existing JSON '{file_path}'. Starting new list.")
             existing_data = []
        except Exception as read_err:
             print(f"(Saver) Error reading existing file {file_path}: {read_err}. Starting new list.")
             existing_data = []
        results_list = [dict(zip(results_dict.keys(), values)) for values in zip(*results_dict.values())]
        existing_data.extend(results_list)
        with open(file_path, 'w') as f:
            json.dump(existing_data, f, indent=4)
        print(f"(Saver) Successfully appended results to {file_path}")
    except Exception as e:
        print(f"(Saver) !!! ERROR saving results to {file_path}: {e}\\n{traceback.format_exc()}")


# --- Setup Logging ---
logger = None
# Use function_bank_path (already defined above) to derive log path
log_file_path = os.path.join(os.path.dirname(function_bank_path), "pipeline_run.log")
output_dir_for_log = os.path.dirname(log_file_path)
if output_dir_for_log and not os.path.exists(output_dir_for_log):
     try:
         os.makedirs(output_dir_for_log, exist_ok=True)
         print(f"Created log directory: {output_dir_for_log}")
     except OSError as e:
         print(f"Warning: Could not create log directory {output_dir_for_log}: {e}")
         log_file_path = "pipeline_run.log" # Fallback

print(f"Setting up logging to file: {log_file_path}")
try:
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file_path),
            logging.StreamHandler()
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    logger.info("Logging configured successfully.")
except Exception as log_e:
     print(f"Error configuring logging: {log_e}. Using basic print statements.")
     class PrintLogger:
         def info(self, msg, *args): print("INFO: " + (msg % args if args else msg))
         def warning(self, msg, *args): print("WARNING: " + (msg % args if args else msg))
         def error(self, msg, *args): print("ERROR: " + (msg % args if args else msg))
         def exception(self, msg, *args): print("EXCEPTION: " + (msg % args if args else msg) + f"\\n{traceback.format_exc()}")
     logger = PrintLogger()


# --- Initialize variables ---
k_overall_metrics = []
k_preprocessing_times = []
k_postprocessing_times = []
k_prediction_times = []
k_metrics_times = []
pipeline_error = None
pipeline_traceback = None
k_preprocess_images_fns = []
k_postprocess_preds_fns = []
k_preprocessed_images = []
k_predictions = []
k_final = []
k_preprocess_images_fns_single_img = []
k_postprocess_preds_fns_single_img = []
k_preprocessed_images_single_img = []
k_preprocessing_times_single_img = []
k_postprocessing_times_single_img = []
k_predictions_single_img = []
k_final_single_img = []
k_prediction_times_single_img = []
k_metrics_times_single_img = []
k_overall_metrics_single_img = []

# --- Generated Preprocessing & Postprocessing Function Definition ---
# This placeholder will be replaced by the executor
{_PREPROCESSING_POSTPROCESSING_FUNCTIONS_PLACEHOLDER}
# --- End of Generated Code ---

try:
    # --- Check if function exists ---
    for i in range(k):
        if f'preprocess_images_{i + 1}' not in globals() or not callable(globals().get(f'preprocess_images_{i + 1}', None)):
            err_msg = f"FATAL: preprocess_images_{i + 1} function not defined correctly by generated code."
            logger.error(err_msg)
            raise NameError(err_msg)
        else:
            k_preprocess_images_fns.append(globals().get(f'preprocess_images_{i + 1}', None))
            k_preprocess_images_fns_single_img.append(globals().get(f'preprocess_images_{i + 1}', None))
            logger.info(f"preprocess_images_{i + 1} function appears to be defined.")

    for i in range(k):
        if f'postprocess_preds_{i + 1}' not in globals() or not callable(globals().get(f'postprocess_preds_{i + 1}', None)):
            err_msg = f"FATAL: postprocess_preds_{i + 1} function not defined correctly by generated code."
            logger.error(err_msg)
            raise NameError(err_msg)
        else:
            k_postprocess_preds_fns.append(globals().get(f'postprocess_preds_{i + 1}', None))
            k_postprocess_preds_fns_single_img.append(globals().get(f'postprocess_preds_{i + 1}', None))
            logger.info(f"postprocess_preds_{i + 1} function appears to be defined.")

    # --- Set random seeds ---
    logger.info(f"Setting random seeds using integer seed: {seed}")
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)

    # --- Initialize segmenter tool ---
    logger.info("Initializing CellposeTool...")
    segmenter = CellposeTool(model_name="cyto3", device=gpu_id)
    logger.info(f"Loading data from {data_path}...")
    raw_images, gt_masks, image_sources = segmenter.loadCombinedDataset(data_path, dataset_size)
    logger.info(f"Data loaded: {len(raw_images)} images.")
    logger.info(f"[Single Image] Loading single image as a tester...")

    # --- Prepare ImageData ---
    images = ImageData(raw=raw_images, batch_size=batch_size, image_ids=image_sources)
    single_image_data = ImageData(raw=raw_images[:2], batch_size=2, masks=gt_masks[:2], image_ids=image_sources[:2])


    for i, preprocess_images in enumerate(k_preprocess_images_fns_single_img):
        logger.info(f"[Single Image] Applying preprocess_images_{i + 1} function...")
        preprocessing_start_time = time.time()
        try:
            k_preprocessed_images_single_img.append(preprocess_images(single_image_data))
            k_preprocessing_times_single_img.append(time.time() - preprocessing_start_time)
            logger.info(f"[Single Image] Preprocessing {i + 1} finished in {k_preprocessing_times_single_img[i]:.2f} seconds.")
            if not isinstance(single_image_data, ImageData):
                raise TypeError(f"Preprocessing function {i + 1} returned type {type(single_image_data)}, expected ImageData.")
        except Exception as preprocess_error:
            logger.error(f"[Single Image] !!! ERROR occurred inside the dynamically generated preprocess_images_{i + 1} function:")
            logger.exception(preprocess_error)
            raise preprocess_error

    # --- [Single Image] Run Segmenter ---
    for i, preprocessed_images in enumerate(k_preprocessed_images_single_img):
        logger.info(f"[Single Image] Running segmentation prediction {i + 1}...")
        prediction_start_time = time.time()
        k_predictions_single_img.append(segmenter.predict(single_image_data, batch_size=single_image_data.batch_size))
        k_prediction_times_single_img.append(time.time() - prediction_start_time)
        logger.info(f"[Single Image] Segmentation prediction finished in {k_prediction_times_single_img[i]:.2f} seconds.")

    # --- [Single Image] Apply Postprocssing ---
    for i, pred in enumerate(k_predictions_single_img):
        logger.info(f"[Single Image] Applying postprocess_preds_{i + 1} function...")
        postprocessing_start_time = time.time()
        try:
            k_final_single_img.append(globals().get(f'postprocess_preds_{i + 1}', lambda x: x)(pred))
            k_postprocessing_times_single_img.append(time.time() - postprocessing_start_time)
            logger.info(f"[Single Image] Postprocessing {i + 1} finished in {k_postprocessing_times_single_img[i]:.2f} seconds.")
        except Exception as postprocess_error:
            logger.error(f"[Single Image] !!! ERROR occurred inside the dynamically generated postprocess_preds_{i + 1} function:")
            logger.exception(postprocess_error)
            raise postprocess_error
    
    # --- [Single Image] Calculate Metrics ---
    for i, pred in enumerate(k_final_single_img):
        logger.info(f"[Single Image] Calculating metrics for prediction {i + 1}...")
        metrics_start_time = time.time()
        cur_single_image_data = ImageData(raw=raw_images[:2], batch_size=2, masks=gt_masks[:2], image_ids=image_sources[:2], predicted_masks=pred)
        k_overall_metrics_single_img.append(segmenter.evaluateDisaggregated(cur_single_image_data))
        k_metrics_times_single_img.append(time.time() - metrics_start_time)
        logger.info(f"[Single Image] Metrics calculation finished in {k_metrics_times_single_img[i]:.2f} seconds.")

    # --- [Single Image] Compute Advantages ---
    if k_overall_metrics_single_img:
        retrieve_reward = lambda m: m["average_precision"]
        k_overall_metrics_single_img = [{**m, "advantage": retrieve_reward(m) - group_baseline} for m in k_overall_metrics]
        
    # --- Apply Preprocessing ---
    for i, preprocess_images in enumerate(k_preprocess_images_fns):
        logger.info(f"Applying preprocess_images_{i + 1} function...")
        preprocessing_start_time = time.time()
        try:
            k_preprocessed_images.append(preprocess_images(images))
            k_preprocessing_times.append(time.time() - preprocessing_start_time)
            logger.info(f"Preprocessing {i + 1} finished in {k_preprocessing_times[i]:.2f} seconds.")
            if not isinstance(images, ImageData):
                raise TypeError(f"Preprocessing function {i + 1} returned type {type(images)}, expected ImageData.")
        except Exception as preprocess_error:
            logger.error(f"!!! ERROR occurred inside the dynamically generated preprocess_images_{i + 1} function:")
            logger.exception(preprocess_error)
            raise preprocess_error

    # --- Run Segmenter ---
    for i, preprocessed_images in enumerate(k_preprocessed_images):
        logger.info(f"Running segmenter prediction {i + 1}...")
        prediction_start_time = time.time()
        k_predictions.append(segmenter.predict(preprocessed_images, batch_size=preprocessed_images.batch_size))
        k_prediction_times.append(time.time() - prediction_start_time)
        logger.info(f"Segmenter prediction finished in {k_prediction_times[i]:.2f} seconds.")

    # --- Apply Postprocssing ---
    for i, pred in enumerate(k_predictions):
        logger.info(f"Applying postprocess_preds_{i + 1} function...")
        postprocessing_start_time = time.time()
        try:
            k_final.append(globals().get(f'postprocess_preds_{i + 1}', lambda x: x)(pred))
            k_postprocessing_times.append(time.time() - postprocessing_start_time)
            logger.info(f"Postprocessing {i + 1} finished in {k_postprocessing_times[i]:.2f} seconds.")
        except Exception as postprocess_error:
            logger.error(f"!!! ERROR occurred inside the dynamically generated postprocess_preds_{i + 1} function:")
            logger.exception(postprocess_error)
            raise postprocess_error

    # --- Calculate Metrics ---
    for i, pred in enumerate(k_final):
        logger.info(f"Calculating metrics for prediction {i + 1}...")
        metrics_start_time = time.time()
        new_images = ImageData(raw=raw_images, batch_size=batch_size, image_ids=image_sources, masks=gt_masks, predicted_masks=pred)
        k_overall_metrics.append(segmenter.evaluateDisaggregated(new_images))
        k_metrics_times.append(time.time() - metrics_start_time)
        logger.info(f"Metrics calculation finished in {k_metrics_times[i]:.2f} seconds.")

    # --- Compute Advantages ---
    if k_overall_metrics and advantage_enabled:
        retrieve_reward = lambda m: m["average_precision"]
        k_overall_metrics = [{**m, "advantage": retrieve_reward(m) - group_baseline} for m in k_overall_metrics]
    
    # --- Log Metrics ---
    if k_overall_metrics:
        avg_precision_values = [metric.get('average_precision', 'N/A') for metric in k_overall_metrics]
        logger.info("All overall metrics: average_precision: %s", avg_precision_values)
    else:
        logger.info("All overall metrics: average_precision: []")

    # --- CHANGE: Save Results ONLY on SUCCESS (End of Try Block) ---
    # Check if metrics were actually calculated (not empty)
    if k_overall_metrics:
        logger.info("Saving successful results to function bank...")
        # Use inspect.getsource to get function source code and rename functions
        k_preprocessing_functions = [
            re.sub(
                r'^(\s*def\s+)preprocess_images_\d+(\s*\()',
                r'\1preprocess_images\2',
                inspect.getsource(func),
                flags=re.MULTILINE
            )
            for func in k_preprocess_images_fns
        ]
        k_postprocessing_functions = [
            re.sub(
                r'^(\s*def\s+)postprocess_preds_\d+(\s*\()',
                r'\1postprocess_preds\2',
                inspect.getsource(func),
                flags=re.MULTILINE
            )
            for func in k_postprocess_preds_fns
        ]
        results_to_save = {
            'preprocessing_function': k_preprocessing_functions,
            'postprocessing_function': k_postprocessing_functions,
            'overall_metrics': k_overall_metrics, # Known to be not None here
            'seed': [seed] * k,
            'timestamp': [script_start_time] * k,
            'gpu_id': [gpu_id] * k,
            'timestamp_finish': [time.time()] * k,
            'preprocessing_time': k_preprocessing_times,
            'prediction_time': k_prediction_times,
            'postprocessing_time': k_postprocessing_times,
            'metrics_time': k_metrics_times,
            'error': [None] * k, # Explicitly None on success
            'traceback': [None] * k # Explicitly None on success
        }
        # Call the save function defined earlier
        save_results_to_json(function_bank_path, results_to_save)
    else:
        # This case shouldn't normally be reached if evaluate() works,
        # but good to log if metrics end up None without an exception.
        logger.warning("Pipeline seemed to complete, but no metrics were generated. Not saving to bank.")

except Exception as e:
     # Errors caught here (from function check, pipeline steps, or re-raised preprocess error)
     pipeline_error = f"Error during pipeline execution: {str(e)}"
     pipeline_traceback = traceback.format_exc()
     logger.error(f"!!! ERROR during pipeline execution:")
     logger.exception(e)

finally:
    # --- CHANGE: Finally block now only handles logging end and exit code ---
    logger.info("--- Run Finished ---")
    # Check if an error was recorded in the except block
    if 'pipeline_error' in locals() and pipeline_error:
        logger.warning("Exiting with error code 1 due to pipeline failure.")
        exit(1)
    # Otherwise, exit normally (exit code 0)